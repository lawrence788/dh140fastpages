{
  
    
        "post0": {
            "title": "What Sentiment Analysis Can Reveal About Common Speech Patterns",
            "content": "Introduction . Project Overview . This project analyzes the sentiment of a movie reviews dataset using Python through Jupyter Notebooks. . Research Question: What common speech patterns can be found in positive and negative commentary? . Our group wanted to explore how people tend to speak when talking in an extremely positive and negative manner, especially in the context of movie reviews. Are there specific words that are frequently used when speaking in a positive sentiment versus a negative one? How often do those words show up in speech? How polarizing are these frequently used words? Do people tend to speak more when they are feeling positive or negative about something? This is the first step that we want to take when analyzing the speech patterns of movie reviews. . Why It Matters . With this information, we can later see whether these patterns match typical human behavior/speech, or if they are more specific in an online, more anonymous environment. We can use our findings to potentially see how polarizing these spaces are and how they affect our society in the long run as this type of analysis can also be used on things like social media comments and news articles where commentary can be even more polarizing. As we explore speech patterns in settings outside of normal, in person conversation, we can get a better idea of how people truly think and feel in certain environments or how people will think and feel given what we already know. . Methods . Data Source . IMDB Dataset of 50K Movie Reviews . More Dataset Information . We are using the IMDB Dataset of 50K Movie Reviews Large Movie Review Dataset from kaggle which contains 50K movie reviews. This dataset can be used for binary sentiment classification as it contains highly polar positive and negative movie reviews for training and testing. . The dataset contains two columns, one which contains the movie review text and the other with its corresponding sentiment categorization of positive or negative. Both the review and sentiment columns are object data types which represent strings. . We also need to acknowledge the shortcomings of our dataset. Although there is not much information on where the reviews are from, we can assume the reviews cover multiple movies. With that being said, we also do not know what time period these reviews are from, the criteria for being described as &quot;highly polar&quot;, or the backgrounds of the people who made these reviews. The context in which these reviews were made would have provided more insight into common behavioral patterns. . Negative Words . Positive Words . In order to identify the positive and negative words, we can easily scan the web for premade datasets. These two shown above are datasets containing negative and positive words, respectively, that we can use to analyze our movie reviews with. These datasets may help us recognize whether a review is actually negative/positive based on the words used, the most common words used in a negative/positive statement, how the context matters when using negative/positive words, and other speech patterns. . Project Scope . The intended analysis is to see what are the common &quot;positive&quot; and &quot;negative&quot; words being used and how often they are used. The resulting visualizations for this would probably be something like a bar chart highlighting the ten most common &quot;positive&quot; and &quot;negative&quot; words and how often they appear in the dataset. We can also how long &quot;positive&quot; reviews are compared to &quot;negative&quot; reviews by checking the word count for each review and plotting the total word count frequency in a separate bar chart. Another analysis we could do is categorize common &quot;positive&quot; and &quot;negative&quot; words to see how polarizing they really are and plot this on another bar chart to visualize the scale of how positive &quot;positive&quot; reviews are and how negative &quot;negative&quot; reviews are. . Analytical Process . We will first start by exploring our dataset as a whole to better understand how we can utilize the data we have to answer our research question. We know that the dataset contains both positive and negative reviews, so we will also explore the positive and negative reviews separately. We&#39;ll conduct a word frequency analysis to see what kinds of words are being used in a negative/positive context. Within our word frequency analysis, we&#39;ll also take a look at the sentiment of each word to understand the vocabulary being used and how they may affect the overall review. When looking at positive and negative reviews as a whole, we&#39;ll calculate a sentiment score on each review to compare the score with the original sentiment categorization to understand the accuracy of our analysis. We&#39;ll also calculate the average positive and negative review sentiment scores and compare them with each other. Lastly, we&#39;ll look into how the length of the reviews affect the strength of the review&#39;s sentiment. . Expected Insights . We expect to see more polarizing &quot;positive&quot; and &quot;negative&quot; reviews to be lengthier in word count as people tend to speak more than they are feeling extremely &quot;positive&quot; or &quot;negative&quot; which is typical in normal, everyday, real life behavior. However, many people tend to not show their true thoughts and feelings when speaking in public, so perhaps in spaces like movie reviews, people&#39;s wording may be more polarizing than if they were to be giving the review in person. In other words, their online, anonymous reviews may be a lot more emotional and biased than what people would typically say aloud. We might see that although online commentary may reflect human behavior in that it represents what people actually are thinking and feeling, it may not be representative of what people would normally say to others. However, organizations can use these findings to gauge a more truthful reaction from the public for their respective initiatives and campaigns to see what they need to improve upon in order to generate a positive response. . Results . import pandas as pd import nltk from nltk.tokenize import word_tokenize, sent_tokenize from nltk.corpus import stopwords from string import punctuation nltk.download(&#39;stopwords&#39;) from nltk.probability import FreqDist from nltk.stem import WordNetLemmatizer from nltk.sentiment import vader nltk.download(&#39;vader_lexicon&#39;) import numpy as np import matplotlib.pyplot as plt nltk.download(&#39;punkt&#39;) . [nltk_data] Downloading package stopwords to /home/jovyan/nltk_data... [nltk_data] Package stopwords is already up-to-date! [nltk_data] Downloading package vader_lexicon to [nltk_data] /home/jovyan/nltk_data... [nltk_data] Package vader_lexicon is already up-to-date! [nltk_data] Downloading package punkt to /home/jovyan/nltk_data... [nltk_data] Package punkt is already up-to-date! . True . imdb_df = pd.read_csv(&#39;IMDB Dataset.csv&#39;) . Overall Word Frequency . Now that we have a better idea of our dataset in that it contains reviews labeled as positive or negative and that there are an equal amount of each, we can separate the two and conduct more in depth exploratory analysis. By taking a closer look at the positive and negative reviews separately, we will be able to set the foundation for our formal analysis. Let&#39;s start by looking at word frequency. . #positiveReviews = [] #negativeReviews = [] positiveText = &quot;&quot; negativeText = &quot;&quot; for i in range(len(imdb_df[&#39;review&#39;])): if imdb_df[&#39;sentiment&#39;][i] == &quot;positive&quot;: #positiveReviews.append(imdb_df[&#39;review&#39;][i]) positiveText += &quot; &quot; + imdb_df[&#39;review&#39;][i] else: #negativeReviews.append(imdb_df[&#39;review&#39;][i]) negativeText += &quot; &quot; + imdb_df[&#39;review&#39;][i] #print(positiveReviews[:5]) #tokenize the words sent = sent_tokenize(positiveText) words = [] for s in sent: for w in word_tokenize(s): words.append(w) #remove stopwords myStopWords = list(punctuation) + stopwords.words(&#39;english&#39;) wordsNoStop = [] for i in words: if i.lower() not in myStopWords: wordsNoStop.append(i) print(&quot;30 most common words in positive reviews&quot;) tmcPos = [] freq = FreqDist(wordsNoStop) for j in sorted(freq, key=freq.get, reverse=True)[:30]: print(j,freq[j]) tmcPos.append(j) positiveFreq = freq #tokenize the words sent = sent_tokenize(negativeText) words = [] for s in sent: for w in word_tokenize(s): words.append(w) #remove stopwords myStopWords = list(punctuation) + stopwords.words(&#39;english&#39;) wordsNoStop = [] for i in words: if i.lower() not in myStopWords: wordsNoStop.append(i) print(&quot;30 most common words in negative reviews&quot;) tmcNeg = [] negfreq = FreqDist(wordsNoStop) for j in sorted(negfreq, key=negfreq.get, reverse=True)[:30]: print(j,negfreq[j]) tmcNeg.append(j) onlyPositive = [] onlyNegative = [] both = set() for myind in range(30): if tmcNeg[myind] not in tmcPos: onlyNegative.append(tmcNeg[myind]) elif tmcPos[myind] not in tmcNeg: onlyPositive.append(tmcPos[myind]) else: both.add(tmcPos[myind]) both.add(tmcNeg[myind]) print(&quot;words that are only commonly used in positive reviews&quot;) print(onlyPositive) print(&quot;words that are only commonly used in negative reviews&quot;) print(onlyNegative) print(&quot;words that are commonly used in both negative and positive reviews&quot;) print(both) #onlyPositive = [] #for pword in freq: # if pword not in negfreq: # onlyPositive.append((pword, freq[pword])) #onlyNegative = [] #for negword in negfreq: # if negword not in freq: # onlyNegative.append((negword, freq[negword])) #onlyPositive.sort(key = lambda x: -1*x[1]) #onlyNegative.sort(key = lambda x: -1*x[1]) #print(&quot;30 most common words in positive reviews&quot;) #print(onlyPositive[:30]) #print(&quot;30 most common words in negative reviews&quot;) #print(onlyNegative[:30]) . 30 most common words in positive reviews br 97952 &#39;s 63028 film 40162 movie 36180 &#39;&#39; 32033 `` 31520 n&#39;t 26486 one 23978 like 16425 good 13826 story 12112 great 11825 time 11623 see 11514 would 11099 really 10446 well 10036 also 9379 much 8777 even 8333 first 8297 people 8178 get 8029 ... 7940 love 7771 best 7587 way 7467 films 7305 many 7089 life 7078 30 most common words in negative reviews br 103996 &#39;s 58740 movie 47633 n&#39;t 39444 film 35779 `` 34504 &#39;&#39; 34060 one 23114 like 21246 would 15033 good 13756 bad 13415 even 13370 really 11821 ... 11805 time 11324 could 11134 see 10351 get 9794 much 9678 story 9510 people 8907 make 8899 made 8346 movies 7798 plot 7696 acting 7659 way 7277 characters 7093 first 7060 words that are only commonly used in positive reviews [&#39;also&#39;, &#39;films&#39;, &#39;life&#39;] words that are only commonly used in negative reviews [&#39;bad&#39;, &#39;could&#39;, &#39;make&#39;, &#39;made&#39;, &#39;movies&#39;, &#39;plot&#39;, &#39;acting&#39;, &#39;characters&#39;] words that are commonly used in both negative and positive reviews {&#39;really&#39;, &#39;like&#39;, &#39;br&#39;, &#39;movie&#39;, &#34;n&#39;t&#34;, &#39;story&#39;, &#39;film&#39;, &#39;get&#39;, &#39;good&#39;, &#39;first&#39;, &#39;even&#39;, &#39;time&#39;, &#34;&#39;&#39;&#34;, &#39;see&#39;, &#34;&#39;s&#34;, &#39;would&#39;, &#39;``&#39;, &#39;...&#39;, &#39;people&#39;, &#39;much&#39;, &#39;one&#39;} . So, what do these words tell us? . Here we found the 30 most common words used in positive reviews and the 30 most common words used in negative reviews. For both, we see that our code is counting &quot;br&#39;, &quot;&#39;s&quot;, and &quot;&#39;&#39;&quot; which aren&#39;t words and symbols that reveal anything of use to our research. We also see more neutral words like &quot;movie&quot; and &quot;film&quot; which provide context rather than reveal much about the sentiment of the review. This tells us that we&#39;ll need to do some cleaning before we can conduct a word frequency analysis that will be meaningful, i.e. if we want to find the top 10 most commonly used words in negative/positive reviews. . If we ignore these &quot;words&quot;, we can take a first look at the other, more meaningful words that were listed. For positive reviews, we can see words like &quot;like&quot;, &quot;good&quot;, &quot;great&quot;, &quot;love&quot;, and &quot;best&quot; which are commonly associated with positive statements. For negative reviews, we only really see &quot;bad&quot; as a word that is commonly associated with negative statements. Interestingly enough, most of the common words found in the negative reviews are words associated with the film itself like &quot;plot&quot;, &quot;story&quot;, &quot;acting&quot;, and &quot;characters&quot; which may be an indication that reviews tends to be more critical in a constructive sense rather than an outburst of how horrible the film was. . Many words are commonly used in both negative and positive reviews. As we can see in the output above, words such as &quot;good&quot;, &quot;film&quot;, &quot;like&quot; are commonly used in both bad and good reviews. This is because words such as &quot;film&quot; are purely descriptive, since all reviews will be talking about the film. Surprisingly, the word &quot;good&quot; shows up in negative reviews as well, although on further analysis one can deduce that this can stem from phrases such as &quot;not very good&quot; that are used negatively. Similarly, even though the word &quot;like&quot; is generally regarded as positive, it can be used negatively as well, such as in the phrase &quot;I didn&#39;t like the film&quot;. Therefore, although many words lean towards good or bad when taken at face value, context matters a lot in their true meaning in reviews. . We also found words that are commonly used specifically in positive reviews and those include &quot;also&quot;, &quot;films&quot;, and &quot;life&quot;. The words that are commonly used specifically in negatived reviews are &quot;bad&quot;, &quot;could&quot;, &quot;make&quot;, &quot;made&quot;, &quot;movies&quot;, &quot;plot&quot;, &quot;acting&quot;, and &quot;characters&quot;. Although we need context to analyze these words since on their own they do not reveal much, it is interesting to note that there are more words in the negative list, which may indicate that more negative reviews tend to be wordier or longer to produce such results. We will need to conduct more analysis to confirm this theory. . Another thing we noticed with this initial word frequency analysis was that the code itself was taking a while to run because the dataset was too large. We originally thought the dataset would be a decent sized sample of reviews, but because of the long runtime, we did not want our analysis to get stuck on a particular cell of code. As a result, we decided to use a smaller sample from the dataset and extrapolate our findings at the end of our analysis. . Expanding upon the previous section where we looked at the 30 most frequent words in positive and negative reviews, we tried to create data visualizations to better represent these differences, using a smaller and cleaner sample of reviews. Let&#39;s first clean out the &quot;words&quot; and symbols that distract from our analysis. . reviews = &quot;&quot; for ind in range(1000): reviews = reviews + &quot; &quot; + imdb_df[&quot;review&quot;][ind] # Create a list of stop words stop_words = stopwords.words(&quot;english&quot;) + list(punctuation) # Tokenize and convert all the reviews to lowercase words = word_tokenize(reviews.lower()) # Remove all unwanted words completewords = [w for w in words if w not in stop_words] # Lemmatize the list of words completestemmed = [WordNetLemmatizer().lemmatize(w) for w in completewords] # Find the frequency of all words freq = FreqDist(completestemmed) # Turn this list into a pandas dataframe wordlist = [] wordfreq = [] for i in sorted(freq, key=freq.get, reverse=True): wordlist.append(i) wordfreq.append(freq[i]) df = pd.DataFrame({&#39;wordlist&#39;:wordlist, &#39;wordfreq&#39;:wordfreq}) # Purge weird words df = df.drop(labels=0) df = df.drop(labels=1) df = df.drop(labels=4) df = df.drop(labels=5) df = df.drop(labels=6) df = df.drop(labels=26) df[:30] . wordlist wordfreq . 2 movie | 2056 | . 3 film | 1770 | . 7 one | 1012 | . 8 like | 785 | . 9 get | 552 | . 10 time | 543 | . 11 character | 532 | . 12 would | 528 | . 13 see | 528 | . 14 good | 524 | . 15 even | 523 | . 16 story | 480 | . 17 make | 467 | . 18 really | 454 | . 19 scene | 443 | . 20 much | 395 | . 21 well | 393 | . 22 way | 361 | . 23 people | 361 | . 24 also | 344 | . 25 first | 343 | . 27 great | 342 | . 28 bad | 342 | . 29 made | 336 | . 30 think | 328 | . 31 could | 325 | . 32 go | 317 | . 33 thing | 304 | . 34 show | 301 | . 35 life | 300 | . Here we have cleaned our dataset so that meaningless letters and symbols will no longer be included in our word frequency analysis. There may be more odd words, but we have cleaned enough so that our smaller sample of data will provide meaningful results. Let&#39;s continue with our exploratory analysis on word frequency. . print(df[&#39;wordfreq&#39;].mean()) # Median print(df[&#39;wordfreq&#39;].median()) # Mode print(df[&#39;wordfreq&#39;].mode()) . 6.483727970591415 1.0 0 1 dtype: int64 . mean6 = df[df[&#39;wordfreq&#39;] == 6] mean7 = df[df[&#39;wordfreq&#39;] == 7] print(mean6[:10]) print(mean7[:10]) . wordlist wordfreq 2821 security 6 2822 stare 6 2823 performed 6 2824 terribly 6 2825 grown 6 2826 love. 6 2827 meaningless 6 2828 mattei 6 2829 vivid 6 2830 loneliness 6 wordlist wordfreq 2543 sold 7 2544 johanson 7 2545 soap 7 2546 portrait 7 2547 connect 7 2548 connected 7 2549 noble 7 2550 davis 7 2551 quit 7 2552 mildly 7 . What do these values tell us? . The mean tells us that the word frequency value is on average 6.5, or since word frequency has to be a whole number, between 6 and 7. This means that the many of the words in this data frame are used a total of 6-7 times which tells us that there are some words people tend to use more often in the context of movie reviews. We can see some of these words above. . The median tells us the middle value for word frequency, and in this case it is 1.0. This tells us that we have a lot of words that are only used once throughout the whole data frame of reviews. . The mode is the most commonly seen word frequency value, and in this case, it is 1. This makes sense because of our median value that tells us there are a lot of words that only appear once. So, the median and the mode tell us that a word frequency of 1 is the most common word frequency value in our data frame of common words. . Now that we have a better understanding of our dataset, we can begin visualizing these word frequency exploration findings using plots. . df.plot.hist(bins=1000, xlim=(0, 150), ylim=(0,2000)) . &lt;AxesSubplot:ylabel=&#39;Frequency&#39;&gt; . This histogram shows the distribution of word frequency values, and we can see that most words are used less than 20 times and only a few words are used hundreds of times. . bar_df = df.loc[0:20, [&quot;wordlist&quot;, &quot;wordfreq&quot;]] bar_df.plot.barh(x=&quot;wordlist&quot;, y=&quot;wordfreq&quot;) #ax = bar_df.plot.bar(x=&#39;wordlist&#39;, y=&#39;wordfreq&#39;, rot=0) . &lt;AxesSubplot:ylabel=&#39;wordlist&#39;&gt; . We can also use a bar chart to visually compare how many times the most common words are being used in our dataset. We can see that the frequency value drastically decreases as we go down the word list and once we reach around the 500 range, the bars are more similar in length. This chart only shows the 17 most frequent words, so if we were to look at more words, we would still be able to see this same type of distribution but at a smaller frequency value. Comparing this chart with the histogram allows us to dive deeper into the word frequency distribution and see what types of words are being used at the highest frequency level. . #this barchart isnt as helpful, should replace with something else bar_df = df.loc[18490:, [&quot;wordlist&quot;, &quot;wordfreq&quot;]] bar_df.plot.barh(x=&quot;wordlist&quot;, y=&quot;wordfreq&quot;) # need to find all words with only a freq of 1 . &lt;AxesSubplot:ylabel=&#39;wordlist&#39;&gt; . This bar chart differs drastically from the previous chart in that all the bars are at the same value of one. This tells us that there may be many words that are not only used just once, but a few times. What may be more meaningful when looking at the lowest word frequency level is to list out the words instead since a bar chart that lists even the 100 least frequent words may all be at the value of 1. . df.loc[df[&#39;wordfreq&#39;] == 1] . wordlist wordfreq . 8816 timid | 1 | . 8817 penitentary | 1 | . 8818 emerald | 1 | . 8819 inwards | 1 | . 8820 privacy | 1 | . ... ... | ... | . 18499 intuition | 1 | . 18500 stubborn | 1 | . 18501 minds. | 1 | . 18502 &#39;swearing | 1 | . 18503 volume. | 1 | . 9688 rows × 2 columns . df.loc[df[&#39;wordfreq&#39;] &lt; 5] . wordlist wordfreq . 3853 me. | 4 | . 3854 hearted | 4 | . 3855 experimental | 4 | . 3856 em | 4 | . 3857 muslim | 4 | . ... ... | ... | . 18499 intuition | 1 | . 18500 stubborn | 1 | . 18501 minds. | 1 | . 18502 &#39;swearing | 1 | . 18503 volume. | 1 | . 14651 rows × 2 columns . Like we predicted, there are over 9,500 words that are used only once, so a bar chart would not be ideal for us to visualize the lower end of the word frequency distribution. When we take a look at the word frequency value of below 5 to gauge how large a bar chart would need to be to see more bar lengths like with the most frequent words, the number of words that would be included is too large for us to properly analyze. . With our cleaned data frame of words, we can run code to retrieve a better list of the most commonly used words in positive and negative reviews. We&#39;ll do that in the Positive Reviews and Negative Reviews Section. Let&#39;s first do a bit more exploratory sentiment analysis to get a better sense of the dataset as a whole. . Overall Word Sentiment . In order to get a better gauge of the most frequently used words that will help with our sentiment analysis, we will also need to ignore words like &quot;movie&quot;, &quot;film&quot;, &quot;characters&quot;, and &quot;scene&quot; that provide more context about the review, are common in both positive and negative reviews, and are more neutral in sentiment. These words will not reveal much about speech patterns in a positively or negatively emotional context. So, let&#39;s find the sentiment values of our most common words to differentiate neutral, positive, extremely positive, negative, and extremely negative words in our dataset. . sia = vader.SentimentIntensityAnalyzer() k = 0 newwordlist = [] words_sentiments = [] for index, row in df.iterrows(): word = df[&quot;wordlist&quot;].iloc[k] #print(word + &quot;&#39;s compound polarity score is &quot; + str(sia.polarity_scores(word)[&quot;compound&quot;])) words_sentiments.append(sia.polarity_scores(word)[&quot;compound&quot;]) newwordlist.append(word) k+=1 words_sentiment_df = pd.DataFrame({&#39;wordlist&#39;:newwordlist, &#39;wordsent&#39;:words_sentiments}) words_sentiment_df[:30] . wordlist wordsent . 0 movie | 0.0000 | . 1 film | 0.0000 | . 2 one | 0.0000 | . 3 like | 0.3612 | . 4 get | 0.0000 | . 5 time | 0.0000 | . 6 character | 0.0000 | . 7 would | 0.0000 | . 8 see | 0.0000 | . 9 good | 0.4404 | . 10 even | 0.0000 | . 11 story | 0.0000 | . 12 make | 0.0000 | . 13 really | 0.0000 | . 14 scene | 0.0000 | . 15 much | 0.0000 | . 16 well | 0.2732 | . 17 way | 0.0000 | . 18 people | 0.0000 | . 19 also | 0.0000 | . 20 first | 0.0000 | . 21 great | 0.6249 | . 22 bad | -0.5423 | . 23 made | 0.0000 | . 24 think | 0.0000 | . 25 could | 0.0000 | . 26 go | 0.0000 | . 27 thing | 0.0000 | . 28 show | 0.0000 | . 29 life | 0.0000 | . k = 0 positive_words = [] negative_words = [] positive_word_sentiments = [] negative_word_sentiments = [] for index, row in words_sentiment_df.iterrows(): word = words_sentiment_df[&quot;wordlist&quot;].iloc[k] if words_sentiment_df[&quot;wordsent&quot;].iloc[k] &gt; .5: #print(word + &quot;&#39;s compound polarity score is &quot; + df[&quot;wordsent&quot;].iloc[k]) positive_word_sentiments.append(sia.polarity_scores(word)[&quot;compound&quot;]) positive_words.append(word) #print(&quot;The most positive words are:&quot;) elif words_sentiment_df[&quot;wordsent&quot;].iloc[k] &lt; -.5: #print(word + &quot;&#39;s compound polarity score is &quot; + df[&quot;wordsent&quot;].iloc[k]) negative_word_sentiments.append(sia.polarity_scores(word)[&quot;compound&quot;]) negative_words.append(word) #print(&quot;The most negative words are:&quot;) k+=1 #print (&quot;The positive words are:&quot; + str(positive_words)) #print (&quot;The negative words are:&quot; + str(negative_words)) positive_df = pd.DataFrame({&#39;wordlist&#39;:positive_words, &#39;wordsent&#39;:positive_word_sentiments}) positive_df = positive_df.sort_values(by=&#39;wordsent&#39;, ascending=False) print (&quot;The positive words are:&quot;) print (positive_df[:30]) negative_df = pd.DataFrame({&#39;wordlist&#39;:negative_words, &#39;wordsent&#39;:negative_word_sentiments}) negative_df = negative_df.sort_values(by=&#39;wordsent&#39;, ascending=True) print (&quot;The negative words are:&quot;) print (negative_df[:30]) . The positive words are: wordlist wordsent 185 magnificently 0.6597 119 sweetheart 0.6486 1 best 0.6369 2 love 0.6369 29 perfectly 0.6369 19 greatest 0.6369 79 freedom 0.6369 81 love. 0.6369 146 &#39;love 0.6369 129 glee 0.6369 102 glorious 0.6369 103 best. 0.6369 112 paradise 0.6369 121 excellence 0.6249 120 joyous 0.6249 23 masterpiece 0.6249 127 excellently 0.6249 162 masterpiece. 0.6249 180 awesome. 0.6249 206 great. 0.6249 208 superb. 0.6249 24 superb 0.6249 0 great 0.6249 21 awesome 0.6249 145 brilliantly. 0.6124 172 brightest 0.6124 55 gorgeous 0.6124 179 perfectness 0.6124 161 heavenly 0.6124 203 splendor 0.6124 The negative words are: wordlist wordsent 119 rapist -0.7096 280 slavery -0.7003 28 rape -0.6908 5 murder -0.6908 6 kill -0.6908 56 terrorist -0.6908 161 fu -0.6908 183 raped -0.6808 271 hell. -0.6808 13 hell -0.6808 184 terrorism -0.6808 46 murderer -0.6808 236 suicidal -0.6705 27 suicide -0.6705 16 killed -0.6705 284 catastrophe -0.6597 211 evil. -0.6597 50 murdered -0.6597 12 killing -0.6597 51 devil -0.6597 10 evil -0.6597 87 horrific -0.6597 186 &#39;villains -0.6597 249 cancer -0.6597 242 killing. -0.6597 30 tragedy -0.6597 125 murdering -0.6486 245 killer. -0.6486 207 terrorize -0.6486 269 heartbroken -0.6486 . We can see that there are many words that are considered positive or negative based on their sentiment. The words with the highest sentiment scores are &quot;magnificently&quot;, &quot;sweetheart&quot;, &quot;best&quot;, &quot;love&quot;, and &quot;perfectly&quot; which mean they are the most positive words being used. The words with the lowest sentiment scores are &quot;rapist&quot;, &quot;slavery&quot;, &quot;rape&quot;, &quot;murder&quot;, and &quot;kill&quot; which mean they are the most negative words being used. When we compare the two lists, it seems like the negative words again are more likely describing the plot of the film rather than one&#39;s feelings about the film. The positive words are more descriptive whereas the negative words are more specific about something concrete, which may mean that when people dislike something, they will more likely talk about what it was they did not like rather than their feelings about not liking it, and be more objective. When people are happier about something, they will more likely talk about their feelings towards whatever it was that made them happy more than the thing itself. . words_sentiment_df.plot.hist(bins=10, xlim=(-1, 1), ylim=(0,2000)) . &lt;AxesSubplot:ylabel=&#39;Frequency&#39;&gt; . When looking at this histogram of word sentiment distribution, most words are neutral but we can see a slight difference between the use of positive and negative words. There are stronger negative words being used and the frequency is more even around the -0.5 sentiment value than the 0.5 sentiment value. This tells us that people will use stronger negative words when talking in a negative context compared to positive words in a positive context. It also tells us that people use more neutral leaning words in positive statements, downplaying their positive emotions more compared to negative statements. Perhaps people tend to be more emotional in a negative sense. . Now that we have a better sense of what we want to look for and the overall dataset, we can conduct more specific analysis on positive and negative reviews separately to compare and contrast the most common words being used, the average sentiment, etc. . Positive Reviews - Word Frequency . In this section, we will conduct word frequency analysis on only the positive reviews to see what type of language is typically used in a positive context. . reviews = &quot;&quot; for ind in range(5000): if imdb_df[&quot;sentiment&quot;][ind] == &quot;positive&quot;: reviews = reviews + &quot; &quot; + imdb_df[&quot;review&quot;][ind] stop_words = stopwords.words(&quot;english&quot;) + list(punctuation) words = word_tokenize(reviews.lower()) completewords = [w for w in words if w not in stop_words] completestemmed = [WordNetLemmatizer().lemmatize(w) for w in completewords] freq = FreqDist(completestemmed) poswordlist = [] poswordfreq = [] for i in sorted(freq, key=freq.get, reverse=True): poswordlist.append(i) poswordfreq.append(freq[i]) pos_df = pd.DataFrame({&#39;wordlist&#39;:poswordlist, &#39;wordfreq&#39;:poswordfreq}) # Clean up the dataframe and remove weird words pos_df = pos_df.drop(labels=0) pos_df = pos_df.drop(labels=1) pos_df = pos_df.drop(labels=4) pos_df = pos_df.drop(labels=5) pos_df = pos_df.drop(labels=7) pos_df = pos_df.drop(labels=26) pos_df = pos_df.drop(labels=31) pos_df[:30] . wordlist wordfreq . 2 film | 4601 | . 3 movie | 4401 | . 6 one | 2718 | . 8 like | 1701 | . 9 time | 1448 | . 10 good | 1423 | . 11 story | 1372 | . 12 character | 1307 | . 13 see | 1263 | . 14 great | 1222 | . 15 well | 1158 | . 16 would | 1126 | . 17 get | 1113 | . 18 make | 1077 | . 19 really | 1031 | . 20 also | 1025 | . 21 much | 960 | . 22 scene | 940 | . 23 show | 923 | . 24 life | 898 | . 25 first | 888 | . 27 way | 866 | . 28 love | 863 | . 29 best | 826 | . 30 people | 795 | . 32 think | 730 | . 33 many | 723 | . 34 year | 699 | . 35 little | 694 | . 36 watch | 693 | . After cleaning out the odd words and symbols, we can see that like in our analysis on the overall dataset, the top words are more neutral and do not reveal much. However, we can see words like &quot;like&quot;, &quot;good&quot;, &quot;great&quot;, &quot;well&quot;, &quot;love&quot;, and &quot;best&quot; amongst the top 30 which tells us that in addition to context words about the film, people use a variety of more positive words in a positive statement. . posbar_df = pos_df.loc[0:50, [&quot;wordlist&quot;, &quot;wordfreq&quot;]] posbar_df.plot.barh(x=&quot;wordlist&quot;, y=&quot;wordfreq&quot;, figsize=(30,30), fontsize=20) . &lt;AxesSubplot:ylabel=&#39;wordlist&#39;&gt; . We can also visualize the word frequency on a bar chart. Most words are around 1000 and below in frequency value which is still a significant number as it tells us that these words are usually what people say when giving a positive review. We can see there is quite a diversity in positive language within the context of movie reviews. . from wordcloud import WordCloud poswordfreq = pos_df[&quot;wordlist&quot;].to_json()[:1000] poswordfreqcloud = WordCloud(background_color=&quot;white&quot;).generate(poswordfreq) #plot the wordcloud plt.figure(figsize = (12, 12)) plt.imshow(poswordfreqcloud) #to remove the axis value plt.axis(&quot;off&quot;) plt.show() . Another way to visualize the word frequency is with a word cloud, and we created one here for the most frequently used positive words with the most frequently used words being the largest in the plot. . poswordsent = pd.Series(positive_df.wordsent.values,index=positive_df.wordlist).to_dict() poswordsentcloud = WordCloud(background_color=&#39;white&#39;, width = 300, height=300, margin=2).generate_from_frequencies(poswordsent) #plot the wordcloud plt.figure(figsize = (12, 12)) plt.imshow(poswordsentcloud) #to remove the axis value plt.axis(&quot;off&quot;) plt.show() . Since there were quite a few neutral words in the word frequency list, we decided it would be more helpful for use to look at the words with the highest sentiment values or the most positive words being used in positive reviews. We can then compare these results with the word frequency analysis. Here we can see that some of the largest words in the word cloud are &quot;magnificently&quot;, &quot;perfectly&quot;, &quot;greatest&quot;, &quot;love&quot;, &quot;best&quot;, and &quot;sweetheart&quot; which is similar to what we found at the start. Comparing this with the word frequency cloud, words like &quot;best&quot; and &quot;love&quot; are relatively large in both clouds, meaning that they are very positive words that are often used in positive reviews. . What does this all mean? . These results tell us that in a positive context, people do use words like &quot;best&quot; and &quot;love&quot; which are strongly positive, descriptive words. They also used these words frequently to describe either their feelings or the subject at hand. . Negative Reviews - Word Frequency . We can do the same analysis for negative reviews and then compare our findings. . reviews = &quot;&quot; for ind in range(5000): if imdb_df[&quot;sentiment&quot;][ind] == &quot;negative&quot;: reviews = reviews + &quot; &quot; + imdb_df[&quot;review&quot;][ind] stop_words = stopwords.words(&quot;english&quot;) + list(punctuation) words = word_tokenize(reviews.lower()) completewords = [w for w in words if w not in stop_words] completestemmed = [WordNetLemmatizer().lemmatize(w) for w in completewords] freq = FreqDist(completestemmed) negwordlist = [] negwordfreq = [] for i in sorted(freq, key=freq.get, reverse=True): negwordlist.append(i) negwordfreq.append(freq[i]) neg_df = pd.DataFrame({&#39;wordlist&#39;:negwordlist, &#39;wordfreq&#39;:negwordfreq}) # Clean up dataframe and drop weird words neg_df = neg_df.drop(labels=0) neg_df = neg_df.drop(labels=1) neg_df = neg_df.drop(labels=4) neg_df = neg_df.drop(labels=5) neg_df = neg_df.drop(labels=6) neg_df = neg_df.drop(labels=19) neg_df[:30] . wordlist wordfreq . 2 movie | 5726 | . 3 film | 4334 | . 7 one | 2584 | . 8 like | 2291 | . 9 even | 1606 | . 10 would | 1580 | . 11 good | 1508 | . 12 bad | 1484 | . 13 character | 1433 | . 14 get | 1431 | . 15 time | 1428 | . 16 make | 1316 | . 17 really | 1226 | . 18 scene | 1155 | . 20 could | 1124 | . 21 see | 1120 | . 22 story | 1112 | . 23 much | 997 | . 24 people | 982 | . 25 thing | 931 | . 26 made | 887 | . 27 first | 873 | . 28 plot | 872 | . 29 well | 845 | . 30 way | 827 | . 31 go | 820 | . 32 look | 776 | . 33 acting | 772 | . 34 know | 746 | . 35 think | 739 | . After cleaning out the odd words and symbols, we can see that like in our analysis on the overall dataset, the top words are again more neutral and do not reveal much. There also are not as many negative words besides &quot;bad&quot;, but there are many words that seem to describe the film itself which again, we found at the start of our analysis. This reinforces the idea that people do not use many descriptive words when talking about their negative feelings towards something because they will talk about the actual subject that caused their negative feelings. This also may show us that when people do start to use descriptive, negative words, they tend to be stronger in sentiment. . negbar_df = neg_df.loc[0:30, [&quot;wordlist&quot;, &quot;wordfreq&quot;]] negbar_df.plot.barh(x=&quot;wordlist&quot;, y=&quot;wordfreq&quot;, figsize=(30,30), fontsize=20) . &lt;AxesSubplot:ylabel=&#39;wordlist&#39;&gt; . We can visualize word frequency on a bar chart like we did with positive reviews, and we see that we get similar results in that most of the most frequently used words have a frequency value of around 1000. . from wordcloud import WordCloud negwords = neg_df[&quot;wordlist&quot;].to_json()[:1000] negwordcloud = WordCloud(background_color=&quot;white&quot;).generate(negwords) #plot the wordcloud plt.figure(figsize = (12, 12)) plt.imshow(negwordcloud) #to remove the axis value plt.axis(&quot;off&quot;) plt.show() . A word cloud also shows us the same results, where the largest words are not necessarily &quot;negative&quot; words, but more about the film. . negwordsent = pd.Series((negative_df.wordsent.values*-1),index=negative_df.wordlist).to_dict() negwordsentcloud = WordCloud(background_color=&#39;white&#39;, width = 300, height=300, margin=2).generate_from_frequencies(negwordsent) #plot the wordcloud plt.figure(figsize = (12, 12)) plt.imshow(negwordsentcloud) #to remove the axis value plt.axis(&quot;off&quot;) plt.show() . In this word cloud, some of the largest words include &quot;rapist&quot;, &quot;slavery&quot;, &quot;murder&quot;, &quot;kill&quot;, and &quot;terrorist&quot; which is consistent with our previous analysis. Again, these words are most likely about a film&#39;s plot rather than the reviewer&#39;s actual feelings towards the film. This tells us that the negatively strongest words being used are usually about the film and that in a negative context, people do not typically use harsh language when saying they dislike a film. However, if they did like a film, they would use positively strong words to convey their feelings. . Comparing this word cloud with the most frequently used negative words cloud, there is not as much overlap like there was with positive reviews. This may be due to the types of movies that were reviewed, given the featured words in the negative sentiment word cloud, but we do not have access to that information so we would be unable to confirm the validity of this theory. . Sentiment Analysis . We can conduct more in depth sentiment analysis with the whole review rather than just the sentiment of specific words to better understand how the words we analyzed in the previous sections are used in the context of a full movie review. . sia = vader.SentimentIntensityAnalyzer() . k = 0 data = [] for index, row in imdb_df.iterrows(): while k &lt; 50: review = imdb_df[&quot;review&quot;].iloc[k] print(imdb_df[&quot;sentiment&quot;].iloc[k]) print(sia.polarity_scores(review)) # print(&#39;Review &#39; + str(k) + &quot;&#39;s compound polarity score is &quot; + str(sia.polarity_scores(review)[&quot;compound&quot;])) data.append(sia.polarity_scores(review)[&quot;compound&quot;]) k+=1 #data[:50] . positive {&#39;neg&#39;: 0.203, &#39;neu&#39;: 0.748, &#39;pos&#39;: 0.048, &#39;compound&#39;: -0.9951} positive {&#39;neg&#39;: 0.053, &#39;neu&#39;: 0.776, &#39;pos&#39;: 0.172, &#39;compound&#39;: 0.9641} positive {&#39;neg&#39;: 0.094, &#39;neu&#39;: 0.714, &#39;pos&#39;: 0.192, &#39;compound&#39;: 0.9605} negative {&#39;neg&#39;: 0.138, &#39;neu&#39;: 0.797, &#39;pos&#39;: 0.065, &#39;compound&#39;: -0.9213} positive {&#39;neg&#39;: 0.052, &#39;neu&#39;: 0.801, &#39;pos&#39;: 0.147, &#39;compound&#39;: 0.9744} positive {&#39;neg&#39;: 0.017, &#39;neu&#39;: 0.758, &#39;pos&#39;: 0.225, &#39;compound&#39;: 0.9828} positive {&#39;neg&#39;: 0.024, &#39;neu&#39;: 0.871, &#39;pos&#39;: 0.104, &#39;compound&#39;: 0.9022} negative {&#39;neg&#39;: 0.149, &#39;neu&#39;: 0.654, &#39;pos&#39;: 0.197, &#39;compound&#39;: 0.8596} negative {&#39;neg&#39;: 0.166, &#39;neu&#39;: 0.662, &#39;pos&#39;: 0.172, &#39;compound&#39;: 0.2362} positive {&#39;neg&#39;: 0.094, &#39;neu&#39;: 0.531, &#39;pos&#39;: 0.375, &#39;compound&#39;: 0.9149} negative {&#39;neg&#39;: 0.084, &#39;neu&#39;: 0.696, &#39;pos&#39;: 0.221, &#39;compound&#39;: 0.9482} negative {&#39;neg&#39;: 0.107, &#39;neu&#39;: 0.779, &#39;pos&#39;: 0.114, &#39;compound&#39;: 0.5223} negative {&#39;neg&#39;: 0.145, &#39;neu&#39;: 0.75, &#39;pos&#39;: 0.105, &#39;compound&#39;: -0.9721} negative {&#39;neg&#39;: 0.086, &#39;neu&#39;: 0.795, &#39;pos&#39;: 0.12, &#39;compound&#39;: 0.3425} positive {&#39;neg&#39;: 0.117, &#39;neu&#39;: 0.713, &#39;pos&#39;: 0.17, &#39;compound&#39;: 0.6168} negative {&#39;neg&#39;: 0.145, &#39;neu&#39;: 0.745, &#39;pos&#39;: 0.109, &#39;compound&#39;: -0.6993} positive {&#39;neg&#39;: 0.113, &#39;neu&#39;: 0.761, &#39;pos&#39;: 0.127, &#39;compound&#39;: 0.3506} negative {&#39;neg&#39;: 0.18, &#39;neu&#39;: 0.778, &#39;pos&#39;: 0.043, &#39;compound&#39;: -0.9868} positive {&#39;neg&#39;: 0.044, &#39;neu&#39;: 0.872, &#39;pos&#39;: 0.084, &#39;compound&#39;: 0.6518} negative {&#39;neg&#39;: 0.055, &#39;neu&#39;: 0.788, &#39;pos&#39;: 0.157, &#39;compound&#39;: 0.945} positive {&#39;neg&#39;: 0.154, &#39;neu&#39;: 0.706, &#39;pos&#39;: 0.141, &#39;compound&#39;: 0.5425} negative {&#39;neg&#39;: 0.198, &#39;neu&#39;: 0.716, &#39;pos&#39;: 0.086, &#39;compound&#39;: -0.9755} positive {&#39;neg&#39;: 0.04, &#39;neu&#39;: 0.794, &#39;pos&#39;: 0.166, &#39;compound&#39;: 0.7948} negative {&#39;neg&#39;: 0.114, &#39;neu&#39;: 0.724, &#39;pos&#39;: 0.162, &#39;compound&#39;: 0.899} negative {&#39;neg&#39;: 0.122, &#39;neu&#39;: 0.797, &#39;pos&#39;: 0.082, &#39;compound&#39;: -0.7602} positive {&#39;neg&#39;: 0.106, &#39;neu&#39;: 0.667, &#39;pos&#39;: 0.227, &#39;compound&#39;: 0.954} positive {&#39;neg&#39;: 0.082, &#39;neu&#39;: 0.748, &#39;pos&#39;: 0.17, &#39;compound&#39;: 0.9962} negative {&#39;neg&#39;: 0.184, &#39;neu&#39;: 0.672, &#39;pos&#39;: 0.144, &#39;compound&#39;: -0.8481} negative {&#39;neg&#39;: 0.24, &#39;neu&#39;: 0.583, &#39;pos&#39;: 0.177, &#39;compound&#39;: -0.9208} positive {&#39;neg&#39;: 0.158, &#39;neu&#39;: 0.745, &#39;pos&#39;: 0.097, &#39;compound&#39;: -0.987} positive {&#39;neg&#39;: 0.143, &#39;neu&#39;: 0.799, &#39;pos&#39;: 0.058, &#39;compound&#39;: -0.9907} positive {&#39;neg&#39;: 0.119, &#39;neu&#39;: 0.781, &#39;pos&#39;: 0.1, &#39;compound&#39;: -0.6485} negative {&#39;neg&#39;: 0.218, &#39;neu&#39;: 0.702, &#39;pos&#39;: 0.08, &#39;compound&#39;: -0.9604} positive {&#39;neg&#39;: 0.039, &#39;neu&#39;: 0.824, &#39;pos&#39;: 0.137, &#39;compound&#39;: 0.9971} negative {&#39;neg&#39;: 0.137, &#39;neu&#39;: 0.715, &#39;pos&#39;: 0.148, &#39;compound&#39;: 0.7133} negative {&#39;neg&#39;: 0.13, &#39;neu&#39;: 0.763, &#39;pos&#39;: 0.107, &#39;compound&#39;: -0.9399} negative {&#39;neg&#39;: 0.221, &#39;neu&#39;: 0.731, &#39;pos&#39;: 0.048, &#39;compound&#39;: -0.962} negative {&#39;neg&#39;: 0.163, &#39;neu&#39;: 0.76, &#39;pos&#39;: 0.077, &#39;compound&#39;: -0.9847} positive {&#39;neg&#39;: 0.05, &#39;neu&#39;: 0.7, &#39;pos&#39;: 0.25, &#39;compound&#39;: 0.9838} negative {&#39;neg&#39;: 0.081, &#39;neu&#39;: 0.747, &#39;pos&#39;: 0.173, &#39;compound&#39;: 0.9797} negative {&#39;neg&#39;: 0.104, &#39;neu&#39;: 0.79, &#39;pos&#39;: 0.106, &#39;compound&#39;: 0.0854} positive {&#39;neg&#39;: 0.03, &#39;neu&#39;: 0.781, &#39;pos&#39;: 0.189, &#39;compound&#39;: 0.9858} negative {&#39;neg&#39;: 0.144, &#39;neu&#39;: 0.732, &#39;pos&#39;: 0.124, &#39;compound&#39;: -0.4678} negative {&#39;neg&#39;: 0.046, &#39;neu&#39;: 0.814, &#39;pos&#39;: 0.14, &#39;compound&#39;: 0.9818} positive {&#39;neg&#39;: 0.026, &#39;neu&#39;: 0.846, &#39;pos&#39;: 0.128, &#39;compound&#39;: 0.9182} positive {&#39;neg&#39;: 0.016, &#39;neu&#39;: 0.891, &#39;pos&#39;: 0.093, &#39;compound&#39;: 0.9401} negative {&#39;neg&#39;: 0.031, &#39;neu&#39;: 0.775, &#39;pos&#39;: 0.194, &#39;compound&#39;: 0.9165} negative {&#39;neg&#39;: 0.054, &#39;neu&#39;: 0.917, &#39;pos&#39;: 0.029, &#39;compound&#39;: -0.68} positive {&#39;neg&#39;: 0.101, &#39;neu&#39;: 0.797, &#39;pos&#39;: 0.102, &#39;compound&#39;: -0.382} negative {&#39;neg&#39;: 0.141, &#39;neu&#39;: 0.695, &#39;pos&#39;: 0.165, &#39;compound&#39;: 0.2152} . Why are some reviews interpreted as positive even though they have negative compound score and vice versa? . Certain reviews are interpreted as positive even though the sentiment value is negative or negative even though the sentiment value is positive. When looking at the polarity scores, we can see that most of the movie reviews have a high neutral score. That means that the movie reviews in this particular dataset are not very polarizing and/or may be looking at both the pros and the cons of the movie, making it more confusing for the Vader analysis to accurately determine the sentiment of the while review. We can look at one of these reviews such as the one at index 0. . &quot;One of the other reviewers has mentioned that after watching just 1 Oz episode you&#39;ll be hooked. They are right, as this is exactly what happened with me. The first thing that struck me about Oz was its brutality and unflinching scenes of violence, which set in right from the word GO. Trust me, this is not a show for the faint hearted or timid. This show pulls no punches with regards to drugs, sex or violence. Its is hardcore, in the classic use of the word. It is called OZ as that is the nickname given to the Oswald Maximum Security State Penitentary. It focuses mainly on Emerald City, an experimental section of the prison where all the cells have glass fronts and face inwards, so privacy is not high on the agenda. Em City is home to many..Aryans, Muslims, gangstas, Latinos, Christians, Italians, Irish and more....so scuffles, death stares, dodgy dealings and shady agreements are never far away. I would say the main appeal of the show is due to the fact that it goes where other shows wouldn&#39;t dare. Forget pretty pictures painted for mainstream audiences, forget charm, forget romance...OZ doesn&#39;t mess around. The first episode I ever saw struck me as so nasty it was surreal, I couldn&#39;t say I was ready for it, but as I watched more, I developed a taste for Oz, and got accustomed to the high levels of graphic violence. Not just violence, but injustice (crooked guards who&#39;ll be sold out for a nickel, inmates who&#39;ll kill on order and get away with it, well mannered, middle class inmates being turned into prison bitches due to their lack of street skills or prison experience) Watching Oz, you may become comfortable with what is uncomfortable viewing....thats if you can get in touch with your darker side.&quot; . This review mentions the brutality of Oz and that even though the reviewer became accustomed to it, they recognize that it is uncomfortable to watch in the beginning. As someone reading this review, we can tell that the reviewer liked the film which is why the sentiment is considered positive in the database. However, Vader sees words such as &quot;uncomfortable&quot;, &quot;brutality&quot;, &quot;not&quot;, &quot;injustice&quot;, &quot;violence&quot;, etc. and decides that this review is leaning towards negative rather than positive. Vader is by no means a perfect algorithm and messes up when reviews are slightly ambiguous such as this one. The next cell will look at how accurate Vader was in analyzing the sentiment of all the movie reviews. . truths = 0 for index, row in imdb_df.iterrows(): review = row[&quot;review&quot;] if sia.polarity_scores(review)[&quot;compound&quot;] &gt; 0 and row[&quot;sentiment&quot;] == &quot;positive&quot;: truths += 1 if sia.polarity_scores(review)[&quot;compound&quot;] &lt; 0 and row[&quot;sentiment&quot;] == &quot;negative&quot;: truths += 1 # Divide the number of true positives and true negatives by the size of the dataframe print(truths/imdb_df.size) . 0.34799 . The Vader sentiment analysis only got an accuracy of 34.799 percent against the IMDB dataset. Despite the poor accuracy, this does not mean Vader isn&#39;t a good algorithm, but that Vader is not powerful enough for this type of dataset that includes more ambiguous language. . We can look at the accuracy of Sentiwordnet next. . from nltk.corpus import sentiwordnet as swn from nltk.tokenize import sent_tokenize, word_tokenize from nltk.corpus import stopwords from string import punctuation myStopWords = list(punctuation) + stopwords.words(&#39;english&#39;) def naiveSentiment(review): reviewPolarity = 0.0 words = [w for w in word_tokenize(review.lower()) if w not in myStopWords] for word in words: sentScore = 0.0 if len(list(swn.senti_synsets(word))) &gt; 0: for i in list(swn.senti_synsets(word)): if i.pos_score() &gt; i.neg_score(): sentScore += i.pos_score() else: sentScore -= i.neg_score() reviewPolarity += sentScore / len(list(swn.senti_synsets(word))) return reviewPolarity truths = 0 for index, row in imdb_df.iterrows(): review = row[&quot;review&quot;] if naiveSentiment(review) &gt; 0 and row[&quot;sentiment&quot;] == &quot;positive&quot;: truths += 1 if naiveSentiment(review) &lt; 0 and row[&quot;sentiment&quot;] == &quot;negative&quot;: truths += 1 print(truths/imdb_df.size) . 0.33101 . The Sentiwordnet algorithm got an accuracy of 33.101 percent so it seems to be slightly worse than the Vader algorithm. However, that might change if we parse the words in each sentence more carefully and remove weird &quot;words&quot; and symbols. . Despite the low accuracy, let&#39;s continue with our sentiment analysis and see what more we can find. . compound_df = pd.DataFrame(data[:50]) #histogram of the compound polarity scores compound_df.plot(kind=&#39;hist&#39;) . &lt;AxesSubplot:ylabel=&#39;Frequency&#39;&gt; . For this dataset, this histogram shows that positive reviews are stronger than negative reviews, and there are more positive reviews with high sentiment values than negative reviews. This tells us that people may use stronger positive language in a positive context and that they use such language more often than when talking in a negative context. . Positive Reviews - Sentiment . We can conduct a closer sentiment analysis by looking at positive and negative reviews separately. Let&#39;s start with the positive reviews. . grouped = imdb_df.groupby(imdb_df.sentiment) positive = grouped.get_group(&quot;positive&quot;) k = 0 posdata = [] for index, row in positive.iterrows(): while k &lt; 50: review = positive[&quot;review&quot;].iloc[k] print(&#39;Review &#39; + str(k) + &quot;&#39;s compound polarity score is &quot; + str(sia.polarity_scores(review)[&quot;compound&quot;])) posdata.append(sia.polarity_scores(review)[&quot;compound&quot;]) k+=1 . Review 0&#39;s compound polarity score is -0.9951 Review 1&#39;s compound polarity score is 0.9641 Review 2&#39;s compound polarity score is 0.9605 Review 3&#39;s compound polarity score is 0.9744 Review 4&#39;s compound polarity score is 0.9828 Review 5&#39;s compound polarity score is 0.9022 Review 6&#39;s compound polarity score is 0.9149 Review 7&#39;s compound polarity score is 0.6168 Review 8&#39;s compound polarity score is 0.3506 Review 9&#39;s compound polarity score is 0.6518 Review 10&#39;s compound polarity score is 0.5425 Review 11&#39;s compound polarity score is 0.7948 Review 12&#39;s compound polarity score is 0.954 Review 13&#39;s compound polarity score is 0.9962 Review 14&#39;s compound polarity score is -0.987 Review 15&#39;s compound polarity score is -0.9907 Review 16&#39;s compound polarity score is -0.6485 Review 17&#39;s compound polarity score is 0.9971 Review 18&#39;s compound polarity score is 0.9838 Review 19&#39;s compound polarity score is 0.9858 Review 20&#39;s compound polarity score is 0.9182 Review 21&#39;s compound polarity score is 0.9401 Review 22&#39;s compound polarity score is -0.382 Review 23&#39;s compound polarity score is 0.9396 Review 24&#39;s compound polarity score is 0.9893 Review 25&#39;s compound polarity score is -0.9274 Review 26&#39;s compound polarity score is 0.7416 Review 27&#39;s compound polarity score is -0.8235 Review 28&#39;s compound polarity score is 0.992 Review 29&#39;s compound polarity score is 0.7747 Review 30&#39;s compound polarity score is 0.6803 Review 31&#39;s compound polarity score is 0.6441 Review 32&#39;s compound polarity score is 0.9813 Review 33&#39;s compound polarity score is 0.9847 Review 34&#39;s compound polarity score is -0.8963 Review 35&#39;s compound polarity score is 0.8498 Review 36&#39;s compound polarity score is 0.9714 Review 37&#39;s compound polarity score is 0.9311 Review 38&#39;s compound polarity score is 0.9943 Review 39&#39;s compound polarity score is 0.9818 Review 40&#39;s compound polarity score is 0.9826 Review 41&#39;s compound polarity score is 0.9831 Review 42&#39;s compound polarity score is 0.8956 Review 43&#39;s compound polarity score is 0.9437 Review 44&#39;s compound polarity score is -0.4877 Review 45&#39;s compound polarity score is 0.9865 Review 46&#39;s compound polarity score is 0.9726 Review 47&#39;s compound polarity score is 0.9761 Review 48&#39;s compound polarity score is 0.9566 Review 49&#39;s compound polarity score is 0.8133 . posdata_df = pd.DataFrame(posdata) posdata_df.plot(kind=&#39;hist&#39;) . &lt;AxesSubplot:ylabel=&#39;Frequency&#39;&gt; . We can see that most of the positive reviews have a positive sentiment but some have a negative sentiment value which we acknowledged earlier when looking at Vader&#39;s accuracy. . Looking at this histogram, we see that most reviews have a very strong sentiment value, thus supporting our idea that people tend to use more positively emotional language rather than neutral language about the film itself. What is a bit surprising is the strength of the positive sentiment and how often these high values are seen since we saw earlier that the negative words being used were much stronger than positive words. We can still see some consistency with the distribution of positive sentiment values since the histogram is not that even, with a larger drop in frequency towards more neutral values. . Negative Reviews - Sentiment . We can now conduct the same analysis on negative reviews and compare our findings . grouped = imdb_df.groupby(imdb_df.sentiment) negative = grouped.get_group(&quot;negative&quot;) k = 0 negdata = [] for index, row in negative.iterrows(): while k &lt; 50: review = negative[&quot;review&quot;].iloc[k] print(&#39;Review &#39; + str(k) + &quot;&#39;s compound polarity score is &quot; + str(sia.polarity_scores(review)[&quot;compound&quot;])) negdata.append(sia.polarity_scores(review)[&quot;compound&quot;]) k+=1 . Review 0&#39;s compound polarity score is -0.9213 Review 1&#39;s compound polarity score is 0.8596 Review 2&#39;s compound polarity score is 0.2362 Review 3&#39;s compound polarity score is 0.9482 Review 4&#39;s compound polarity score is 0.5223 Review 5&#39;s compound polarity score is -0.9721 Review 6&#39;s compound polarity score is 0.3425 Review 7&#39;s compound polarity score is -0.6993 Review 8&#39;s compound polarity score is -0.9868 Review 9&#39;s compound polarity score is 0.945 Review 10&#39;s compound polarity score is -0.9755 Review 11&#39;s compound polarity score is 0.899 Review 12&#39;s compound polarity score is -0.7602 Review 13&#39;s compound polarity score is -0.8481 Review 14&#39;s compound polarity score is -0.9208 Review 15&#39;s compound polarity score is -0.9604 Review 16&#39;s compound polarity score is 0.7133 Review 17&#39;s compound polarity score is -0.9399 Review 18&#39;s compound polarity score is -0.962 Review 19&#39;s compound polarity score is -0.9847 Review 20&#39;s compound polarity score is 0.9797 Review 21&#39;s compound polarity score is 0.0854 Review 22&#39;s compound polarity score is -0.4678 Review 23&#39;s compound polarity score is 0.9818 Review 24&#39;s compound polarity score is 0.9165 Review 25&#39;s compound polarity score is -0.68 Review 26&#39;s compound polarity score is 0.2152 Review 27&#39;s compound polarity score is -0.968 Review 28&#39;s compound polarity score is -0.3696 Review 29&#39;s compound polarity score is -0.9477 Review 30&#39;s compound polarity score is 0.7648 Review 31&#39;s compound polarity score is 0.9139 Review 32&#39;s compound polarity score is -0.3699 Review 33&#39;s compound polarity score is -0.168 Review 34&#39;s compound polarity score is -0.4442 Review 35&#39;s compound polarity score is -0.6416 Review 36&#39;s compound polarity score is 0.9902 Review 37&#39;s compound polarity score is 0.7073 Review 38&#39;s compound polarity score is 0.9422 Review 39&#39;s compound polarity score is 0.9837 Review 40&#39;s compound polarity score is 0.9852 Review 41&#39;s compound polarity score is 0.9382 Review 42&#39;s compound polarity score is -0.9973 Review 43&#39;s compound polarity score is 0.4082 Review 44&#39;s compound polarity score is -0.9504 Review 45&#39;s compound polarity score is -0.8837 Review 46&#39;s compound polarity score is 0.5907 Review 47&#39;s compound polarity score is -0.8489 Review 48&#39;s compound polarity score is -0.9123 Review 49&#39;s compound polarity score is -0.1012 . negdata_df = pd.DataFrame(negdata) negdata_df.plot(kind=&#39;hist&#39;) . &lt;AxesSubplot:ylabel=&#39;Frequency&#39;&gt; . This histogram is very different from our positive review sentiment histogram. Like the positive reviews, we can see that there are mismatches between labels and sentiment values in that we have positive sentiment values for negative reviews. Unlike the positive reviews, we see the mismatches more frequently in negative reviews. We also see there are more neutral leaning sentiment values in negative reviews. This histogram raises a lot of questions as to why the distribution is as it is, which tells us that analysis on the words of the reviews alone will not be sufficient and we will need to look deeper into the context of how the language in these reviews was actually used. . We can also calculate the average sentiment values of positive and negative reviews and compare the two. . n=1 r = np.arange(n) width = 0.25 x = [posdata_df.mean()[0], negdata_df.mean()[0]] ax1 = plt.subplot() ax1.set_xticks([1,2]) # plt.bar(r, posdata_df.mean(), color = &#39;b&#39;, # width = width, edgecolor = &#39;black&#39;, # label=&#39;average positive sentiment&#39;) # plt.bar(r + width, negdata_df.mean(), color = &#39;r&#39;, # width = width, edgecolor = &#39;black&#39;, # label=&#39;average negative sentiment&#39;) plt.bar([1,2], x) ax1.set_xticklabels([&#39;average positive sentiment&#39;, &#39;average negative sentiment&#39;]) plt.ylabel(&quot;Average Sentiment Value&quot;) plt.title(&quot;Average Sentiment Values&quot;) plt.text(1, 0.6, str(round(x[0], 3))) plt.text(2, 0.02, str(round(x[1], 3))) plt.show() . Interestingly enough, this is the opposite of what we saw when we looked at the sentiments of individual words. When we conducted sentiment analysis on individual words, we saw that negative words were stronger in sentiment value than positive words, but here with full reviews, we can see that the average positive review sentiment value is stronger than the average negative sentiment value. This suggests that the language in positive reviews is being used in a way that makes the whole review show a much more positive expression whereas the language in negative reviews tries to make the whole review seem more neutral. If this is the case, this does reflect human behavior in some sense as people tend to be more expressive when speaking positively to be supportive, and are more neutral when speaking negatively to try and avoid conflict. . rlength = [] polarity = [] limit = 1000 for ind in range(len(imdb_df[&quot;review&quot;])): if ind &gt; limit: break review = imdb_df[&quot;review&quot;][ind] rlength.append(len(imdb_df[&quot;review&quot;][ind])) polarity.append(abs(sia.polarity_scores(review)[&quot;compound&quot;])) #print(len(imdb_df[&quot;review&quot;][ind])) #print(abs(sia.polarity_scores(review)[&quot;compound&quot;])) plt.scatter(rlength, polarity) plt.show() . One of our introductory questions was with regard to how people might be more voluminous depending on the strength of the sentiment value, so can we make conclusions about that from the data? . Yes, the plot above shows review length vs. polarity. Review length is measured in number of words in the x-axis, and polarity is measured as an absolute value in the y-axis. As we can see, very long reviews generally have very high polarity scores, regardless of whether the scores are negative or positive. However, some short reviews are also highly polar, and exhibit the same high absolute values for their polarity scores. Therefore, although we cannot make an absolute correlation that short reviews are usually not as polar as long reviews, we can definitely see that most of the time, very long reviews show a high polarity score. This make sense intuitively, since most people wouldn&#39;t care to take such a large effort in writing long reviews unless they really loved or really hated the movie. . Discussion . Our analysis shows that although words individually may suggest one thing, the context in which they are used must be taken into account in order to more accurately determine the sentiment and how people really feel/think about things. We saw how our word analysis suggested that people do not use negative words often, but when they do the words they used are very strong. However, when we looked at the context in which these words were used, we realized that usually the overall statement tends to be more neutral. On the positive side of things, our word analysis showed us that people often used a diverse vocabulary of positive words that were all around the same value in sentiment, but our analysis of the whole statement in which they were used revealed that these words are used to enhance the positive nature of what the reviewer is saying, thus providing a high positive review sentiment value. . Our word frequency and sentiment analysis revealed that people tend to talk about how they felt when speaking positively about something or someone. When talking negatively about something or someone, they tend to discuss the actual thing or person in question instead. This may be because of people&#39;s tendencies to strive for perfection and thus they start giving critiques or constructive criticism which is more neutral than if they were to talk about their negative emotions towards someone/something. It could also be because people want to avoid conflict and thus most people tend to not talk as negatively about someone/something compared, but will be extremely positive if they have something good to say. . These ideas are also supported by our sentiment analysis of the whole movie reviews. Our histograms showed us that positive reviews usually have a strong positive sentiment value whereas negative reviews can be more spread across the board, and thus include more neutral values. This difference is more apparent in the average positive and negative review sentiment values where the negative average was closer to a neutral value. . We also saw that there was some sort of correlation between review length and polarity as longer reviews tended to have stronger sentiment values. This may be because when people feel strongly about someone/something, they tend to speak more about it regardless of whether their feelings are positive or negative. If they felt more neutral about someone/something, then they would not be compelled to say as much. . These findings strengthen what we already know about common speech patters. In general, people want to be more positive because no one wants to be the person to sound overly critical and extremely negative. It is more socially acceptable for people to be overly positive than overly negative, but this can lead people to sugarcoating and hiding what they really feel or think in a negative context, or over exaggerating what they feel or think in a positive context. . Another point to consider for future analysis is more context. To better understand these speech patterns, it would be more helpful for us to understand what movie people were watching, the context in which they were both watching and writing their review, their personal preferences, how their speech patterns online versus in person differ, etc. Our analysis was a decent start at unraveling all the nuances that come with human behavior. . Team Member Contributions . Crystal Huynh wrote most of the narrative throughout the notebook, reorganized and cleaned up the work from Project 2, added wordclouds, wrote code to find polarising words, and adressed some of the feedback from the professor about the histograms. . Larry Qu wrote code to address many feedback points from Project 2, wrote code to make graphs for review length vs polarity, created word frequency plots, analyzed the movie review dataset with the positive and negative word datasets, and analyzed sentiment strength. . Nelson Truong wrote code address many feedback points from Project 2, created word frequency bar charts, cleaned up the dataframes to improve our research and analysis, wrote code for vader polarity and senitwordnet analysis, and calculated the accuracy ofsentiment scores. .",
            "url": "https://lawrence788.github.io/dh140fastpages/2022/03/14/FinalBlog.html",
            "relUrl": "/2022/03/14/FinalBlog.html",
            "date": " • Mar 14, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Assignment 9",
            "content": "Pandas and plotting exercises . In Week 2, you used a dataset from the CORGIS website. You may have used either the Python, CSV, or JSON data files. . For this assignment, use the CSV file format for the same category of data that you used previously. . # Use pandas read_csv function to import the data into a dataframe variable myDf = pd.read_csv(&#39;billionaires.csv&#39;) #read the csv into a dataframe . . print(&quot;rows: &quot; + str(myDf.shape[0])) print(&quot;columns: &quot; + str(myDf.shape[1])) . rows: 2614 columns: 22 . print(list(myDf.columns)) . [&#39;name&#39;, &#39;rank&#39;, &#39;year&#39;, &#39;company.founded&#39;, &#39;company.name&#39;, &#39;company.relationship&#39;, &#39;company.sector&#39;, &#39;company.type&#39;, &#39;demographics.age&#39;, &#39;demographics.gender&#39;, &#39;location.citizenship&#39;, &#39;location.country code&#39;, &#39;location.gdp&#39;, &#39;location.region&#39;, &#39;wealth.type&#39;, &#39;wealth.worth in billions&#39;, &#39;wealth.how.category&#39;, &#39;wealth.how.from emerging&#39;, &#39;wealth.how.industry&#39;, &#39;wealth.how.inherited&#39;, &#39;wealth.how.was founder&#39;, &#39;wealth.how.was political&#39;] . print(myDf.dtypes) . name object rank int64 year int64 company.founded int64 company.name object company.relationship object company.sector object company.type object demographics.age int64 demographics.gender object location.citizenship object location.country code object location.gdp float64 location.region object wealth.type object wealth.worth in billions float64 wealth.how.category object wealth.how.from emerging bool wealth.how.industry object wealth.how.inherited object wealth.how.was founder bool wealth.how.was political bool dtype: object . print(myDf.head(2)) . name rank year company.founded company.name company.relationship 0 Bill Gates 1 1996 1975 Microsoft founder 1 Bill Gates 1 2001 1975 Microsoft founder company.sector company.type demographics.age demographics.gender ... 0 Software new 40 male ... 1 Software new 45 male ... location.gdp location.region wealth.type wealth.worth in billions 0 8.100000e+12 North America founder non-finance 18.5 1 1.060000e+13 North America founder non-finance 58.7 wealth.how.category wealth.how.from emerging wealth.how.industry 0 New Sectors True Technology-Computer 1 New Sectors True Technology-Computer wealth.how.inherited wealth.how.was founder wealth.how.was political 0 not inherited True True 1 not inherited True True [2 rows x 22 columns] . print(myDf.tail(2)) . name rank year company.founded company.name 2612 Zdenek Bakala 1565 2014 1994 Patria Finance 2613 Zhu Wenchen 1565 2014 1999 Furen Pharmaceutical Group company.relationship company.sector company.type demographics.age 2612 founder coal new 53 2613 chairman pharmaceuticals new 48 demographics.gender ... location.gdp location.region 2612 male ... 0.0 Europe 2613 male ... 0.0 East Asia wealth.type wealth.worth in billions wealth.how.category 2612 privatized and resources 1.0 Resource Related 2613 executive 1.0 New Sectors wealth.how.from emerging wealth.how.industry wealth.how.inherited 2612 True Mining and metals not inherited 2613 True Technology-Medical not inherited wealth.how.was founder wealth.how.was political 2612 True True 2613 True True [2 rows x 22 columns] . print(myDf.describe()) . rank year company.founded demographics.age count 2614.000000 2614.000000 2614.000000 2614.000000 mean 599.672533 2008.411630 1924.711936 53.341239 std 467.885695 7.483598 243.776546 25.333320 min 1.000000 1996.000000 0.000000 -42.000000 25% 215.000000 2001.000000 1936.000000 47.000000 50% 430.000000 2014.000000 1963.000000 59.000000 75% 988.000000 2014.000000 1985.000000 70.000000 max 1565.000000 2014.000000 2012.000000 98.000000 location.gdp wealth.worth in billions count 2.614000e+03 2614.000000 mean 1.769103e+12 3.531943 std 3.547083e+12 5.088813 min 0.000000e+00 1.000000 25% 0.000000e+00 1.400000 50% 0.000000e+00 2.000000 75% 7.250000e+11 3.500000 max 1.060000e+13 76.000000 . print(myDf[[&quot;name&quot;]]) . name 0 Bill Gates 1 Bill Gates 2 Bill Gates 3 Warren Buffett 4 Warren Buffett ... ... 2609 Wu Chung-Yi 2610 Wu Xiong 2611 Yang Keng 2612 Zdenek Bakala 2613 Zhu Wenchen [2614 rows x 1 columns] . myDf[[&quot;company.founded&quot;]].plot() . &lt;AxesSubplot:&gt; . print(myDf[[&quot;company.founded&quot;]].loc[[0,1,2,3,4,5,6,7,8,9]]) . company.founded 0 1975 1 1975 2 1975 3 1962 4 1962 5 1990 6 1896 7 1975 8 1975 9 1976 . # as well as the matching 10 elements of a different column that has interesting text print(myDf[[&quot;company.founded&quot;, &quot;name&quot;]].loc[[0,1,2,3,4,5,6,7,8,9]]) #print(myDf[&quot;name&quot;].loc[[0,1,2,3,4,5,6,7,8,9]]) . company.founded name 0 1975 Bill Gates 1 1975 Bill Gates 2 1975 Bill Gates 3 1962 Warren Buffett 4 1962 Warren Buffett 5 1990 Carlos Slim Helu 6 1896 Oeri Hoffman and Sacher 7 1975 Paul Allen 8 1975 Amancio Ortega 9 1976 Lee Shau Kee . # and make a bar plot with the text values horizontally and the numeric values as the bar heights a = myDf[[&quot;company.founded&quot;, &quot;name&quot;]].iloc[:10] #print(a) a.plot(kind=&quot;bar&quot;, x=&quot;name&quot;, y=&quot;company.founded&quot;) . &lt;AxesSubplot:xlabel=&#39;name&#39;&gt; . a.plot(kind=&quot;barh&quot;, x=&quot;name&quot;, y=&quot;company.founded&quot;) . &lt;AxesSubplot:ylabel=&#39;name&#39;&gt; . # and change at least two aesthetic elements (colors, labels, titles, ...) b = a.plot(kind=&quot;barh&quot;, x=&quot;name&quot;, y=&quot;company.founded&quot;, figsize=(12,5)) b.set_title(&quot;Company found dates for several billionaires&quot;) . Text(0.5, 1.0, &#39;Company found dates for several billionaires&#39;) . Free form section . Choose another type of plot that interests you from the pandas.DataFrame.plot documentation [look at the &#39;kind&#39; parameter] and make a new plot of your dataset values using the plot type | . c = myDf.loc[myDf[&#39;year&#39;] == 1996][[&quot;wealth.worth in billions&quot;, &quot;name&quot;]].iloc[:10] #print(c) d = c.plot(kind=&quot;pie&quot;, y=&quot;wealth.worth in billions&quot;) d.legend(c[&quot;name&quot;]) . &lt;matplotlib.legend.Legend at 0x7f4f04e1fb20&gt; . Copy some of your analysis from the Week 2 assignment into new cells below | Clean them up if desired, and make sure that you translate them to work with your new pandas dataframe structure here if needed | Create several plots to complement and extend your analysis | . count = 0 male = 0 nonmale = 0 #4. for loop for index, row in myDf.iterrows(): #3. conditional expression if count &gt;= 100: break if row[&#39;demographics.gender&#39;] == &quot;male&quot;: #2. mathematical operation male += 1 else: nonmale += 1 count += 1 #print(male, nonmale) #5. function definition: compare men vs non-men billionaires def makeOutput(numMale, numNonMale): return (&quot;Out of the top 100 billionaires, there are &quot; + str(numMale) + &quot; male and &quot; + str(numNonMale) + &quot; non-male.&quot;) #5. function execution print(makeOutput(male, nonmale)) . Out of the top 100 billionaires, there are 82 male and 18 non-male. . mydict = {&quot;gender&quot;: [&quot;male&quot;, &quot;non-male&quot;], &quot;count&quot;: [male, nonmale] } f = pd.DataFrame(mydict) #print(f) e = f.plot(kind=&quot;pie&quot;, y=&quot;count&quot;, labels=f[&quot;count&quot;], title=&quot;How many of the top 100 billionaires are men?&quot;) e.legend(f[&quot;gender&quot;]) . &lt;matplotlib.legend.Legend at 0x7f4f04bf77c0&gt; . e = f.plot(kind=&quot;bar&quot;, x=&quot;gender&quot;, y=&quot;count&quot;, title=&quot;How many of the top 100 billionaires are men?&quot;) . As we can see from the charts above, 82 of the top 100 billionaires are men. These two plots above make it much easier to visualize the gender disparity. .",
            "url": "https://lawrence788.github.io/dh140fastpages/fastpages/jupyter/2022/03/02/Assignment03.html",
            "relUrl": "/fastpages/jupyter/2022/03/02/Assignment03.html",
            "date": " • Mar 2, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://lawrence788.github.io/dh140fastpages/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://lawrence788.github.io/dh140fastpages/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://lawrence788.github.io/dh140fastpages/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://lawrence788.github.io/dh140fastpages/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}